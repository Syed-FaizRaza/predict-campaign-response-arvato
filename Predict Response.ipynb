{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T13:53:22.352210Z",
     "start_time": "2019-09-19T13:53:21.050383Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from helpers import CleanData, KFoldCrossVal, feature_selection, feature_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T13:53:25.912775Z",
     "start_time": "2019-09-19T13:53:23.030563Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaner = CleanData()\n",
    "cleaner.label_dict = joblib.load('intermediate/label_dict.pkl')\n",
    "\n",
    "train = pd.read_csv('input/train.csv')\n",
    "train = cleaner.clean(train, label_encode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Model\n",
    "10-time 5-fold cross-validation with Random Forest and no missing value handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T13:53:36.576314Z",
     "start_time": "2019-09-19T13:53:36.543355Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gb_clf = xgb.XGBClassifier(early_stopping_rounds=5)\n",
    "validator = KFoldCrossVal(n_repeats=1)\n",
    "validator.fit(train, gb_clf)\n",
    "\n",
    "validator.best_features.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T13:53:49.260241Z",
     "start_time": "2019-09-19T13:53:46.505782Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('input/test.csv')\n",
    "test = cleaner.clean(test, label_encode=True)\n",
    "\n",
    "def make_submission(clf, df):\n",
    "    pd.DataFrame({'LNR':test.index,\n",
    "                  'RESPONSE': clf.predict_proba(df)[:,1]}).to_csv('submission.csv', index=False)\n",
    "    \n",
    "make_submission(validator.best_est, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T14:02:18.485406Z",
     "start_time": "2019-09-19T14:02:18.439397Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "# import joblib\n",
    "# br_imputer = joblib.load('intermediate/br_imputer.pkl')\n",
    "# imputed_list = joblib.load('intermediate/br_cols_imputed_list.pkl')\n",
    "\n",
    "# train_imputed = train.copy()\n",
    "# train_imputed[imputed_list] = br_imputer.transform(train_imputed[imputed_list])\n",
    "\n",
    "# from sklearn.impute import IterativeImputer\n",
    "# from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "# azdias = pd.read_csv('input/azdias.csv')\n",
    "# azdias = cleaner.clean(azdias, label_encode=True)\n",
    "# azdias_imputed = azdias.copy()\n",
    "# del azdias\n",
    "# azdias_imputed[imputed_list] = br_imputer.transform(azdias_imputed[imputed_list])\n",
    "\n",
    "# br2 = BayesianRidge() #I forgot to impute `EINGEFUEGT_AM` in the first imputer\n",
    "# imputer2 = IterativeImputer(br2) \n",
    "\n",
    "# azdias_imputed[imputed_list+['EINGEFUEGT_AM']] = imputer2.fit_transform(azdias_imputed[imputed_list+['EINGEFUEGT_AM']])\n",
    "\n",
    "# def create_time_difference_variables(df):\n",
    "#     time_cols = ['GEBURTSJAHR', 'EINGEZOGENAM_HH_JAHR','EINGEFUEGT_AM', 'MIN_GEBAEUDEJAHR']\n",
    "#     for i in range(len(time_cols)-1):\n",
    "#         for j in range(i+1, len(time_cols)):\n",
    "#             df['diff_'+time_cols[i]+time_cols[j]] = df[time_cols[i]] - df[time_cols[j]]\n",
    "#     return df\n",
    "# azdias_imputed = create_time_difference_variables(azdias_imputed)\n",
    "# azdias_imputed.isnull().sum().sum()\n",
    "\n",
    "# train_imputed[imputed_list+['EINGEFUEGT_AM']] = imputer2.transform(train_imputed[imputed_list+['EINGEFUEGT_AM']])\n",
    "# train_imputed = create_time_difference_variables(train_imputed)\n",
    "# train_imputed.isnull().sum().sum()\n",
    "\n",
    "# for c in azdias_imputed.columns:\n",
    "#     # restrain the imputed values to be within the range of known values\n",
    "#     if train[c].isnull().sum()==0:\n",
    "#         continue\n",
    "#     train_imputed[c] = np.clip(train_imputed[c], a_min=train[c].min(), a_max=train[c].max())\n",
    "#     azdias_imputed[c] = np.clip(azdias_imputed[c], a_min=train[c].min(), a_max=train[c].max())\n",
    "\n",
    "# azdias_imputed.to_csv('intermediate/azdias_imputed.csv', index=False)\n",
    "# train_imputed.to_csv('intermediate/train_imputed.csv', index=False)\n",
    "\n",
    "azdias_imputed = pd.read_csv('intermediate/azdias_imputed.csv')\n",
    "train_imputed = pd.read_csv('intermediate/train_imputed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T13:54:03.557351Z",
     "start_time": "2019-09-19T13:54:03.422179Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn.ensemble import RandomTreesEmbedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run different models on features selected from Benchmark Model\n",
    "Use the features that appear in the top 20 most important 20 out of 50 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T13:54:16.313818Z",
     "start_time": "2019-09-19T13:54:16.279820Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_features = pd.read_csv('intermediate/top20_features_50_run.csv')\n",
    "selected_features = list(selected_features.feature[selected_features.frequency>=5])\n",
    "\n",
    "selected_features_union = list(set(selected_features + \n",
    "                                   ['D19_SOZIALES',\n",
    "                                    'KBA05_MAXBJ',\n",
    "                                    'D19_FREIZEIT',\n",
    "                                    'KBA05_AUTOQUOT',\n",
    "                                    'KBA05_ALTER2',\n",
    "                                    'KBA13_VORB_3',\n",
    "                                    'KBA05_KRSHERST3',\n",
    "                                    'D19_BANKEN_ANZ_12',\n",
    "                                    'AGER_TYP',\n",
    "                                    'KBA13_BJ_2000',\n",
    "                                    'D19_VERSAND_OFFLINE_DATUM',\n",
    "                                    'SEMIO_KAEM',\n",
    "                                    'diff_EINGEFUEGT_AMMIN_GEBAEUDEJAHR',]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T13:55:17.443889Z",
     "start_time": "2019-09-19T13:55:17.413341Z"
    }
   },
   "outputs": [],
   "source": [
    "validator = KFoldCrossVal()\n",
    "\n",
    "log_clf = LogisticRegression(class_weight='balanced', random_state=7, solver='lbfgs')\n",
    "nb_clf = BernoulliNB()\n",
    "rf_clf = xgb.XGBRFClassifier(scale_pos_weight=80, n_jobs=-1, n_estimators=50, )\n",
    "gb_clf = xgb.XGBClassifier(scale_pos_weight=80, n_jobs=-1, early_stopping_rounds=10, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T14:00:58.207310Z",
     "start_time": "2019-09-19T13:56:36.682407Z"
    }
   },
   "outputs": [],
   "source": [
    "for clf in [rf_clf, gb_clf]:\n",
    "    print(clf.__module__)\n",
    "    validator.fit(train[selected_features+['RESPONSE']], clf, get_best_features=False)\n",
    "    print('std', np.std(validator.test_evals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T14:36:16.199340Z",
     "start_time": "2019-09-19T14:31:46.297464Z"
    }
   },
   "outputs": [],
   "source": [
    "for clf in [log_clf, nb_clf]:\n",
    "    print(clf.__module__)\n",
    "    selected_features_cats = [v for v in selected_features if v in cleaner.categorical_cols]\n",
    "    train_imputed_dummied = pd.get_dummies(train_imputed[selected_features+['RESPONSE']], drop_first=True, \n",
    "                                           columns=selected_features_cats)\n",
    "    validator.fit(train_imputed_dummied, clf, get_best_features=False)\n",
    "    print('std', np.std(validator.test_evals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Run models on PCA features\n",
    "Performs really bad :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T12:28:07.557910Z",
     "start_time": "2019-09-15T12:25:59.210121Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "estimators = [('scaler', MinMaxScaler()),\n",
    "              ('linear_pca', PCA(random_state=7, n_components=248))]\n",
    "print('Making pipeline...')\n",
    "process_pipe = Pipeline(estimators)\n",
    "azdias_dummied = pd.get_dummies(azdias, columns=cleaner.categorical_cols, drop_first=True)\n",
    "print('Fitting pipeline...')\n",
    "process_pipe.fit(azdias_dummied)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T12:40:19.363731Z",
     "start_time": "2019-09-15T12:40:18.884451Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_imputed_dummied = pd.get_dummies(train_imputed, columns=cleaner.categorical_cols, drop_first=True)\n",
    "train_imputed_dummied['TITEL_KZ_2.0'] = 0\n",
    "train_imputed_dummied_pca = pd.DataFrame(process_pipe\n",
    "                                         .transform(train_imputed_dummied[azdias_dummied.columns]),\n",
    "                                         index=train_imputed_dummied.index\n",
    "                                        )\n",
    "train_imputed_dummied_pca['RESPONSE'] = train_imputed_dummied.RESPONSE\n",
    "\n",
    "train_imputed_dummied_pca['RESPONSE'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Tree-embedding features\n",
    "\n",
    "#### Tree embedding features trained on `train` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T03:23:59.151633Z",
     "start_time": "2019-09-20T03:23:59.114267Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T03:33:03.177943Z",
     "start_time": "2019-09-20T03:30:40.975565Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "rf_clf = xgb.XGBRFClassifier(max_depth=3, min_child_weight=30, n_estimators=50,\n",
    "                n_jobs=-1, scale_pos_weight=80)\n",
    "grd_enc = OneHotEncoder(categories='auto', drop='first')\n",
    "lm = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "nb = BernoulliNB()\n",
    "\n",
    "skf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=7)\n",
    "\n",
    "X = train.drop(columns=['RESPONSE'])\n",
    "y = train.RESPONSE\n",
    "\n",
    "\n",
    "def train_tree_embed(X, y, clf, grd_enc, lm, nb, fitted=False):\n",
    "    log_test_results = []\n",
    "    nb_test_results = []\n",
    "    i=0\n",
    "    best_auc = 0\n",
    "    train_auc = None\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        print(i)\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        if not fitted:\n",
    "            clf.fit(X_train, y_train)\n",
    "        grd_enc.fit(clf.apply(pd.concat((X_train, X_test))))\n",
    "        X_train_enc = grd_enc.transform(clf.apply(X_train))\n",
    "        lm.fit(X_train_enc, y_train)\n",
    "        nb.fit(X_train_enc, y_train)\n",
    "\n",
    "        X_test_enc = grd_enc.transform(clf.apply(X_test))\n",
    "        y_pred_lm = lm.predict_proba(X_test_enc)[:, 1]\n",
    "        y_pred_nb = nb.predict_proba(X_test_enc)[:, 1]\n",
    "\n",
    "        res_log = roc_auc_score(y_test, y_pred_lm)\n",
    "        res_nb = roc_auc_score(y_test, y_pred_nb)\n",
    "        if res_log > best_auc:\n",
    "            best_auc = res_log\n",
    "            train_auc = roc_auc_score(y_train, lm.predict_proba(X_train_enc)[:, 1])\n",
    "            best_est = lm\n",
    "        if res_nb > best_auc:\n",
    "            best_auc = res_nb\n",
    "            train_auc = roc_auc_score(y_train, nb.predict_proba(X_train_enc)[:, 1])\n",
    "            best_est = nb\n",
    "        print('Log:',res_log, '   Nb:', res_nb)\n",
    "        log_test_results.append(res_log)\n",
    "        nb_test_results.append(res_nb)\n",
    "        i+=1\n",
    "    print('Log:', np.mean(log_test_results), '        std:', np.std(log_test_results))\n",
    "    print('NB:', np.mean(nb_test_results), '        std:', np.std(nb_test_results))\n",
    "    print(best_est, best_auc, train_auc)\n",
    "    return log_test_results, nb_test_results, clf, grd_enc, best_est, best_auc, train_auc\n",
    "\n",
    "log_test_results, nb_test_results, rf_clf, grd_enc, best_est, best_auc, train_auc = train_tree_embed(X, y, rf_clf, grd_enc, lm, nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:37:30.201336Z",
     "start_time": "2019-09-20T08:36:46.240486Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train[selected_features]\n",
    "y = train.RESPONSE\n",
    "\n",
    "log_test_results, nb_test_results, rf_clf, grd_enc, best_est, best_auc, train_auc = train_tree_embed(X, y, rf_clf, grd_enc, lm, nb)\n",
    "print(np.mean(log_test_results))\n",
    "print(np.mean(nb_test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:39:18.645303Z",
     "start_time": "2019-09-20T08:37:30.203659Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train[selected_features]\n",
    "y = train.RESPONSE\n",
    "\n",
    "_, _, _, _, best_est, best_auc, train_auc = train_tree_embed(X, y, gb_clf, grd_enc, lm, nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T08:55:10.358344Z",
     "start_time": "2019-09-20T08:39:18.647803Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train.drop(columns=['RESPONSE'])\n",
    "y = train.RESPONSE\n",
    "\n",
    "_, _, _, _, best_est, best_auc, train_auc = train_tree_embed(X, y, gb_clf, grd_enc, lm, nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-embedding features trained on `azdias` and `customers`\n",
    "- Concatenate `azdias` and `customers`\n",
    "- Create new variable `RESPONSE`: 1 for `customers` and 0 for `azdias`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T12:17:10.849153Z",
     "start_time": "2019-09-20T12:10:31.994309Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "azdias = pd.read_csv('input/azdias.csv')\n",
    "customers = pd.read_csv('input/customers.csv')\n",
    "\n",
    "customers = customers[azdias.columns]\n",
    "\n",
    "azdias = cleaner.clean(azdias, label_encode=True)\n",
    "customers = cleaner.clean(customers, label_encode=True)\n",
    "\n",
    "azdias['RESPONSE'] = 0\n",
    "customers['RESPONSE'] = 1\n",
    "\n",
    "full = pd.concat((azdias, customers))\n",
    "del azdias\n",
    "del customers\n",
    "from sklearn.model_selection import train_test_split\n",
    "gb_clf_full = xgb.XGBClassifier(scale_pos_weight=2, n_jobs=-1, n_estimators=30, early_stopping_rounds=10)\n",
    "\n",
    "full = full.sample(frac=1, random_state=7)\n",
    "\n",
    "X = full.drop(columns=['RESPONSE'])#[selected_features_union]#\n",
    "y = full.RESPONSE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "gb_clf_full.fit(X_train, y_train, early_stopping_rounds=10,\n",
    "                eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "                eval_metric='auc',\n",
    "                verbose=True\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T12:18:12.143539Z",
     "start_time": "2019-09-20T12:17:10.855071Z"
    }
   },
   "outputs": [],
   "source": [
    "grd_enc = OneHotEncoder(categories='auto', drop='first')\n",
    "grd_lm = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "nb = BernoulliNB()\n",
    "\n",
    "skf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\n",
    "X = train.drop(columns=['RESPONSE'])\n",
    "y = train.RESPONSE\n",
    "\n",
    "log_test_results, nb_test_results, gb_clf_full, grd_enc, best_est, best_auc, train_auc = train_tree_embed(X, y, gb_clf_full, grd_enc, lm, nb, fitted=True)\n",
    "\n",
    "print(np.mean(log_test_results))\n",
    "print(np.mean(nb_test_results))\n",
    "print(best_est, best_auc, train_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T12:22:33.806824Z",
     "start_time": "2019-09-20T12:21:17.544421Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gb_clf_full_reduced = xgb.XGBClassifier(scale_pos_weight=2, n_jobs=-1, n_estimators=30, early_stopping_rounds=10)\n",
    "\n",
    "full = full.sample(frac=1, random_state=7)\n",
    "\n",
    "X = full[selected_features_union]\n",
    "y = full.RESPONSE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "gb_clf_full_reduced.fit(X_train, y_train, early_stopping_rounds=5,\n",
    "                eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "                eval_metric='auc',\n",
    "                verbose=True\n",
    "                )\n",
    "\n",
    "X = train[selected_features_union]\n",
    "y = train.RESPONSE\n",
    "\n",
    "log_test_results, nb_test_results, gb_clf_full, grd_enc, best_est, best_auc, train_auc = train_tree_embed(X, y, \n",
    "                                                                                                          gb_clf_full_reduced, grd_enc, lm, nb, fitted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T12:57:34.260138Z",
     "start_time": "2019-09-20T12:55:48.108871Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_clf_full_reduced = xgb.XGBRFClassifier(scale_pos_weight=1, n_jobs=-1, n_estimators=50)\n",
    "\n",
    "X = full.drop(columns=['RESPONSE'])\n",
    "y = full.RESPONSE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "rf_clf_full_reduced.fit(X_train, y_train, early_stopping_rounds=5,\n",
    "                eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "                eval_metric='auc',\n",
    "                verbose=True\n",
    "                )\n",
    "\n",
    "X = train.drop(columns=['RESPONSE'])\n",
    "y = train.RESPONSE\n",
    "\n",
    "log_test_results, nb_test_results, rf_clf_full_reduced, grd_enc, best_est, best_auc, train_auc = train_tree_embed(X, y, \n",
    "                                                                                                          rf_clf_full_reduced, grd_enc, lm, nb, fitted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T12:18:27.875236Z",
     "start_time": "2019-09-20T12:11:03.558Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train[selected_features_union]\n",
    "y = train.RESPONSE\n",
    "\n",
    "log_test_results = []\n",
    "log_train_results = []\n",
    "nb_test_results = []\n",
    "nb_train_results = []\n",
    "\n",
    "clf = gb_clf_full_reduced\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "    grd_enc.fit(clf.apply(X_train))\n",
    "    X_train_enc = grd_enc.fit_transform(clf.apply(X_train))\n",
    "    lm.fit(X_train_enc, y_train)\n",
    "    nb.fit(X_train_enc, y_train)\n",
    "\n",
    "    X_test_enc = grd_enc.transform(clf.apply(X_test))\n",
    "\n",
    "    res_log = roc_auc_score(y_test, lm.predict_proba(X_test_enc)[:, 1])\n",
    "    train_auc_log = roc_auc_score(y_train, lm.predict_proba(X_train_enc)[:, 1])\n",
    "    performance_log = res_log**2*train_auc_log\n",
    "    \n",
    "    res_nb = roc_auc_score(y_test, nb.predict_proba(X_test_enc)[:, 1])\n",
    "    train_auc_nb = roc_auc_score(y_train, nb.predict_proba(X_train_enc)[:, 1])\n",
    "    performance_nb = res_nb**2*train_auc_nb\n",
    "        \n",
    "    log_test_results.append(res_log)\n",
    "    log_train_results.append(train_auc_log)\n",
    "    nb_test_results.append(res_nb)\n",
    "    nb_train_results.append(train_auc_nb)\n",
    "    \n",
    "log_results = pd.DataFrame({'log_test_results': log_test_results, 'log_train_results': log_train_results})\n",
    "nb_results = pd.DataFrame({'nb_test_results': nb_test_results, 'nb_train_results': nb_train_results})\n",
    "\n",
    "print('Log:', np.mean(log_test_results), '        std:', np.std(log_test_results))\n",
    "print('NB:', np.mean(nb_test_results), '        std:', np.std(nb_test_results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T12:57:34.265835Z",
     "start_time": "2019-09-20T12:55:50.757Z"
    }
   },
   "outputs": [],
   "source": [
    "roc_auc_score(train.RESPONSE, rf_clf_full_reduced.predict_proba(train.drop(columns=['RESPONSE']))[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampling\n",
    "Find most similar negative instances to each positive instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T13:25:16.822061Z",
     "start_time": "2019-09-20T13:24:54.049862Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "train_yes = train[train.RESPONSE==1][selected_features].fillna(-3000)\n",
    "train_no = train[train.RESPONSE==0][selected_features].fillna(-3000)\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "kdB = KDTree(train_no.values, leafsize=20)\n",
    "print(kdB.query(train_yes.values[:1], k=3)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T13:26:31.673763Z",
     "start_time": "2019-09-20T13:25:16.823834Z"
    }
   },
   "outputs": [],
   "source": [
    "train_no_match = list(set(kdB.query(train_yes.values, k=5)[-1].reshape(-1)))\n",
    "train_no_match = train.iloc[train_no_match]\n",
    "train_no_match['RESPONSE'] = 0\n",
    "\n",
    "train_yes = train[train.RESPONSE==1]\n",
    "train_yes['RESPONSE'] = 1\n",
    "\n",
    "train_reduced = pd.concat((train_no_match, train_yes)).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T13:27:04.625049Z",
     "start_time": "2019-09-20T13:26:31.678392Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_clf_reduced = xgb.XGBRFClassifier(random_state=7, n_jobs=-1, scale_pos_weight=5)\n",
    "validator.fit(train_reduced, rf_clf_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T13:27:04.678048Z",
     "start_time": "2019-09-20T13:27:04.627233Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_features2 = [u for u,v in validator.best_features.most_common() if v>=5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T13:27:05.088031Z",
     "start_time": "2019-09-20T13:27:04.680455Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "log_pip = Pipeline([('scale', MinMaxScaler()), ('log', LogisticRegression())])\n",
    "selected_features_union_reduced = list(set(selected_features)|set(selected_features2))\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_reduced[selected_features_union_reduced], \n",
    "                                                        train_reduced.RESPONSE, stratify= train_reduced.RESPONSE,\n",
    "                                                        test_size=0.2, random_state=i,\n",
    "                                                       )\n",
    "    log_pip.fit(X_train, y_train)\n",
    "    print(roc_auc_score(y_test, log_pip.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T12:57:50.877378Z",
     "start_time": "2019-09-20T12:57:50.766085Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T13:24:17.780235Z",
     "start_time": "2019-09-20T13:02:28.694348Z"
    }
   },
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    "    'max_depth':np.arange(3,7),#4\n",
    "    'min_child_weight':np.arange(10, 100, 10), #25\n",
    "    'n_estimators':np.arange(10, 100, 10), #50\n",
    "    'scale_pos_weight':[1, 5, 10, 20, 30, 40, 60,80, 100],\n",
    "    'reg_alpha': [0.001, 0.01, 0.1, 1],\n",
    "    'learning_rate': [1, 0.3, 0.1, 0.01, .005],\n",
    "    'colsample_bytree': [0.2, 0.5, 0.8, 1],\n",
    "    'subsample': np.arange(7, 11, 1)/10,\n",
    "    'gamma': [0.01, 0.1, 0.3, 0.5, 1, 1.5, 2]\n",
    "}\n",
    "\n",
    "gsearch1 = RandomizedSearchCV(estimator = xgb.XGBRFClassifier(learning_rate=0.1, \n",
    "                                                    n_estimators=50, max_depth=3,\n",
    "                                                    gamma=0, subsample=1, \n",
    "                                                    objective= 'binary:logistic', \n",
    "                                                    n_jobs=-1, scale_pos_weight=1, \n",
    "                                                    ),\n",
    "                              param_distributions=param_test1,\n",
    "                              scoring='roc_auc',\n",
    "                              n_iter=200,\n",
    "                              cv=5, \n",
    "                              n_jobs=-1,\n",
    "                              random_state=7,\n",
    "                              return_train_score=True\n",
    "                             )\n",
    "\n",
    "from time import time\n",
    "start = time()\n",
    "gsearch1.fit(train.drop(columns=['RESPONSE']),train['RESPONSE'])\n",
    "print(\"RandomizedSearchCV took %.2f seconds\" % (time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T13:06:48.891287Z",
     "start_time": "2019-09-16T13:06:48.829766Z"
    }
   },
   "outputs": [],
   "source": [
    "gsearch1_res = pd.DataFrame(gsearch1.cv_results_)\n",
    "gsearch1_res.groupby('param_max_depth').mean_test_score.agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T13:08:47.447082Z",
     "start_time": "2019-09-16T13:08:47.395611Z"
    }
   },
   "outputs": [],
   "source": [
    "gsearch1_res.groupby('param_scale_pos_weight').mean_test_score.agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T13:09:14.625029Z",
     "start_time": "2019-09-16T13:09:14.581480Z"
    }
   },
   "outputs": [],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T13:45:39.643799Z",
     "start_time": "2019-09-20T13:28:40.331201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 1019.26 seconds\n"
     ]
    }
   ],
   "source": [
    "gsearch2 = RandomizedSearchCV(estimator = xgb.XGBRFClassifier(learning_rate =0.1, \n",
    "                                                    n_estimators=50, max_depth=3,\n",
    "                                                    gamma=0, subsample=1, \n",
    "                                                    objective= 'binary:logistic', \n",
    "                                                    n_jobs=-1, scale_pos_weight=1, \n",
    "                                                    ),\n",
    "                              param_distributions=param_test1,\n",
    "                              scoring='roc_auc',\n",
    "                              n_iter=200,\n",
    "                              cv=5, \n",
    "                              n_jobs=-1,\n",
    "                              random_state=7,\n",
    "                              return_train_score=True\n",
    "                             )\n",
    "\n",
    "\n",
    "start = time()\n",
    "gsearch2.fit(train[selected_features_union_reduced],train['RESPONSE'])\n",
    "print(\"RandomizedSearchCV took %.2f seconds\" % (time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T13:50:24.922850Z",
     "start_time": "2019-09-20T13:50:24.815053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     mean  count\n",
      "param_subsample                 \n",
      "0.7              0.746499     52\n",
      "0.8              0.759631     53\n",
      "0.9              0.754869     39\n",
      "1.0              0.749529     56\n",
      "                            mean  count\n",
      "param_scale_pos_weight                 \n",
      "1                       0.669326     23\n",
      "5                       0.765430     29\n",
      "10                      0.766513     20\n",
      "20                      0.764664     15\n",
      "30                      0.763614     27\n",
      "40                      0.763712     18\n",
      "60                      0.761698     27\n",
      "80                      0.763066     18\n",
      "100                     0.758013     23\n",
      "                     mean  count\n",
      "param_reg_alpha                 \n",
      "0.001            0.761937     50\n",
      "0.010            0.762446     51\n",
      "0.100            0.759358     46\n",
      "1.000            0.727922     53\n",
      "                        mean  count\n",
      "param_n_estimators                 \n",
      "20                  0.756748     21\n",
      "30                  0.742690     26\n",
      "40                  0.759014     26\n",
      "50                  0.761310     19\n",
      "60                  0.743908     28\n",
      "70                  0.763764     30\n",
      "80                  0.744217     27\n",
      "90                  0.750209     23\n",
      "                            mean  count\n",
      "param_min_child_weight                 \n",
      "10                      0.758652     23\n",
      "20                      0.761671     28\n",
      "30                      0.761622     15\n",
      "40                      0.759547     17\n",
      "50                      0.754442     23\n",
      "60                      0.747344     18\n",
      "70                      0.736120     31\n",
      "80                      0.752423     24\n",
      "90                      0.747490     21\n",
      "                     mean  count\n",
      "param_max_depth                 \n",
      "3                0.750954     45\n",
      "4                0.751464     55\n",
      "5                0.747708     48\n",
      "6                0.759202     52\n",
      "                         mean  count\n",
      "param_learning_rate                 \n",
      "0.005                0.750876     45\n",
      "0.010                0.748984     38\n",
      "0.100                0.754699     43\n",
      "0.300                0.749932     44\n",
      "1.000                0.759734     30\n",
      "                 mean  count\n",
      "param_gamma                 \n",
      "0.01         0.750668     24\n",
      "0.10         0.753055     33\n",
      "0.30         0.752421     35\n",
      "0.50         0.746161     29\n",
      "1.00         0.751933     29\n",
      "1.50         0.749368     23\n",
      "2.00         0.763340     27\n",
      "                            mean  count\n",
      "param_colsample_bytree                 \n",
      "0.2                     0.736653     47\n",
      "0.5                     0.764419     57\n",
      "0.8                     0.755131     57\n",
      "1.0                     0.750125     39\n"
     ]
    }
   ],
   "source": [
    "param_results = pd.DataFrame(gsearch2.cv_results_)\n",
    "for col in gsearch2.cv_results_:\n",
    "    if col.startswith('param_'):\n",
    "        print(param_results.groupby(col)['mean_test_score'].agg(['mean', 'count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T13:47:45.348507Z",
     "start_time": "2019-09-20T13:47:45.258771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.8, 'scale_pos_weight': 10, 'reg_alpha': 1, 'n_estimators': 20, 'min_child_weight': 70, 'max_depth': 5, 'learning_rate': 0.005, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
      "0.7738409200929489\n"
     ]
    }
   ],
   "source": [
    "print(gsearch2.best_params_)\n",
    "\n",
    "print(gsearch2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T14:23:27.473618Z",
     "start_time": "2019-09-20T14:15:31.178483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 476.23 seconds\n"
     ]
    }
   ],
   "source": [
    "param_test2 = {\n",
    "    'min_child_weight':np.arange(50, 150, 10),\n",
    "    'n_estimators':np.arange(10, 100, 10),\n",
    "    'reg_alpha': [0.001, 0.003, 0.01, 0.1, 1, 1.3, 1.5],\n",
    "    'learning_rate': [0.01, .005, 0.001, 0.0005],\n",
    "    'colsample_bytree': np.arange(2, 11, 1)/10,\n",
    "    'subsample': np.arange(7, 11, 1)/10,\n",
    "    'gamma': [0.01, 0.1, 0.3, 0.5]\n",
    "}\n",
    "gsearch3 = RandomizedSearchCV(estimator = xgb.XGBRFClassifier(learning_rate =0.1, \n",
    "                                                    n_estimators=30, max_depth=5,\n",
    "                                                    subsample=1, \n",
    "                                                    min_child_weight= 40,          \n",
    "                                                    objective= 'binary:logistic', \n",
    "                                                    n_jobs=-1, scale_pos_weight=10,          \n",
    "                                                    ),\n",
    "                              param_distributions=param_test2,\n",
    "                              scoring='roc_auc',\n",
    "                              n_iter=100,\n",
    "                              cv=5, \n",
    "                              n_jobs=-1,\n",
    "                              random_state=7,\n",
    "                              return_train_score=True\n",
    "                             )\n",
    "\n",
    "\n",
    "start = time()\n",
    "gsearch3.fit(train[selected_features_union_reduced],train['RESPONSE'])\n",
    "print(\"RandomizedSearchCV took %.2f seconds\" % (time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T14:48:00.829211Z",
     "start_time": "2019-09-20T14:48:00.783690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.8, 'reg_alpha': 1.3, 'n_estimators': 20, 'min_child_weight': 100, 'learning_rate': 0.005, 'gamma': 0.3, 'colsample_bytree': 0.7}\n",
      "0.025005587440342206\n"
     ]
    }
   ],
   "source": [
    "print(gsearch3.best_params_)\n",
    "\n",
    "print(gsearch3.cv_results_['std_test_score'][88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T14:42:08.041340Z",
     "start_time": "2019-09-20T14:42:07.741755Z"
    }
   },
   "outputs": [],
   "source": [
    "make_submission(gsearch3.best_estimator_, test[selected_features_union_reduced])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T14:55:48.703011Z",
     "start_time": "2019-09-20T14:55:48.045972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: report.pdf (deflated 22%)\n",
      "updating: report.ipynb (deflated 74%)\n",
      "updating: README.md (deflated 48%)\n",
      "updating: proposal.pdf (deflated 2%)\n",
      "updating: helpers.py (deflated 70%)\n",
      "  adding: intermediate/br_cols_imputed_list.pkl (deflated 59%)\n",
      "  adding: intermediate/br_imputer.pkl (deflated 32%)\n",
      "  adding: intermediate/features.pkl (deflated 49%)\n",
      "  adding: intermediate/label_dict.pkl (deflated 75%)\n",
      "  adding: intermediate/top20_features_50_run.csv (deflated 57%)\n",
      "  adding: intermediate/transfer_learning.pkl (deflated 41%)\n"
     ]
    }
   ],
   "source": [
    "!zip capstone.zip report.pdf report.ipynb README.md proposal.pdf helpers.py intermediate/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
